{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeee970c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.combine import SMOTETomek\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report, average_precision_score, precision_recall_curve, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "132156bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Load datasets\n",
    "file_paths = [\n",
    "    '/Users/parthgajera/Documents/Thesis_Data/LI-Medium_Trans.csv',\n",
    "    '/Users/parthgajera/Documents/Thesis_Data/LI-Small_Trans.csv'\n",
    "]\n",
    "\n",
    "datasets = [pd.read_csv(fp) for fp in file_paths]\n",
    "\n",
    "# Rename column 'Is Laundering' to 'is_laundering'\n",
    "datasets[0].rename(columns={'Is Laundering': 'is_laundering'}, inplace=True)\n",
    "datasets[1].rename(columns={'Is Laundering': 'is_laundering'}, inplace=True)\n",
    "\n",
    "\n",
    "# %% Feature Engineering Function\n",
    "def feature_engineering(df):\n",
    "    df = df.copy()\n",
    "    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    df['Hour'] = df['Timestamp'].dt.hour\n",
    "    df['DayOfWeek'] = df['Timestamp'].dt.dayofweek\n",
    "    df['SameBank'] = (df['From Bank'] == df['To Bank']).astype(int)\n",
    "    df['SameAccount'] = (df['Account'] == df['To Bank']).astype(int)\n",
    "    df['CurrencyMismatch'] = (df['Receiving Currency'] != df['Payment Currency']).astype(int)\n",
    "    df['Txn Count From Account'] = df.groupby('Account')['Timestamp'].transform('count')\n",
    "    df['Rolling Avg From Account'] = df.sort_values(by='Timestamp') \\\n",
    "        .groupby('Account')['Amount Paid'].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
    "    \n",
    "    # Ensure 'is_laundering' is retained in the dataset\n",
    "    if 'is_laundering' in df.columns:\n",
    "        return df\n",
    "    else:\n",
    "        raise KeyError(\"Column 'is_laundering' is missing from the input data.\")\n",
    "\n",
    "# %% Features for Isolation Forest and Hybrid Model\n",
    "iso_features = [\n",
    "    'Amount Received', 'Amount Paid',\n",
    "    'SameBank', 'SameAccount', 'CurrencyMismatch',\n",
    "    'Txn Count From Account', 'Rolling Avg From Account',\n",
    "    'Payment Format'\n",
    "]\n",
    "\n",
    "hybrid_features = [\n",
    "    'Amount Received', 'Amount Paid',\n",
    "    'SameBank', 'SameAccount', 'CurrencyMismatch',\n",
    "    'Txn Count From Account', 'Rolling Avg From Account',\n",
    "    'anomaly_score'\n",
    "]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['Payment Format']),\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31b492a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of identical rows in both datasets: 0\n"
     ]
    }
   ],
   "source": [
    "# testing the datasets if they have a same data or not\n",
    "\n",
    "df1_raw = datasets[0].copy()\n",
    "df2_raw = datasets[1].copy()\n",
    "\n",
    "df1_raw = df1_raw.reindex(sorted(df1_raw.columns), axis=1)\n",
    "df2_raw = df2_raw.reindex(sorted(df2_raw.columns), axis=1)\n",
    "\n",
    "common_rows = pd.merge(df1_raw.drop_duplicates(), df2_raw.drop_duplicates(), how='inner')\n",
    "\n",
    "print(f\"Number of identical rows in both datasets: {len(common_rows)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4e5921c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Timestamp', 'From Bank', 'Account', 'To Bank', 'Account.1',\n",
      "       'Amount Received', 'Receiving Currency', 'Amount Paid',\n",
      "       'Payment Currency', 'Payment Format', 'is_laundering'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Check the column names in the dataset\n",
    "print(datasets[0].columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf789090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% Training Isolation Forest\n",
    "train_df = feature_engineering(datasets[0])\n",
    "\n",
    "# Prepare features for Isolation Forest (excluding 'is_laundering')\n",
    "X_train_iso = train_df[iso_features]\n",
    "\n",
    "# Isolation Forest Pipeline\n",
    "iso_pipeline = Pipeline([\n",
    "    ('preprocess', preprocessor),\n",
    "    ('iso_forest', IsolationForest(\n",
    "        n_estimators=100,\n",
    "        max_samples=50000,\n",
    "        contamination=0.02,\n",
    "        max_features=1.0,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "iso_pipeline.fit(X_train_iso)\n",
    "\n",
    "# Add anomaly scores and labels\n",
    "X_train_iso_transformed = iso_pipeline.named_steps['preprocess'].transform(X_train_iso)\n",
    "train_df['anomaly_score'] = iso_pipeline.named_steps['iso_forest'].decision_function(X_train_iso_transformed)\n",
    "train_df['anomaly_label'] = iso_pipeline.named_steps['iso_forest'].predict(X_train_iso_transformed)\n",
    "train_df['anomaly_label'] = train_df['anomaly_label'].map({1: 'Normal', -1: 'Anomaly'})\n",
    "train_df['hybrid_label'] = train_df['anomaly_label'].map({'Anomaly': 1, 'Normal': 0})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2455d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Hybrid Data\n",
    "\n",
    "X_hybrid = train_df[hybrid_features]\n",
    "y_hybrid = train_df['is_laundering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c6729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After downsampling:\n",
      "is_laundering\n",
      "0    312354\n",
      "1     16041\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for training the hybrid modeL\n",
    "X_hybrid = train_df[hybrid_features]\n",
    "y_hybrid = train_df['is_laundering']\n",
    "\n",
    "# Downsampling based on 'is_laundering'\n",
    "minority_df = train_df[train_df['is_laundering'] != 0]\n",
    "majority_df = train_df[train_df['is_laundering'] == 0].sample(frac=0.01, random_state=42)\n",
    "\n",
    "hybrid_sample_df = pd.concat([minority_df, majority_df])\n",
    "\n",
    "X_hybrid_sample = hybrid_sample_df[hybrid_features]\n",
    "y_hybrid_sample = hybrid_sample_df['is_laundering']\n",
    "\n",
    "print(f\"After downsampling:\\n{y_hybrid_sample.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e46c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After SMOTETomek resampling:\n",
      "is_laundering\n",
      "0    298428\n",
      "1    142251\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SMOTETomek for resampling\n",
    "smote_tomek = SMOTETomek(random_state=42, sampling_strategy=0.5)\n",
    "X_resampled, y_resampled = smote_tomek.fit_resample(X_hybrid_sample, y_hybrid_sample)\n",
    "\n",
    "print(f\"After SMOTETomek resampling:\\n{y_resampled.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f7a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative Skip SMOTETomek\n",
    "X_resampled = X_hybrid_sample\n",
    "y_resampled = y_hybrid_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e702a10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After SMOTETomek resampling:\n",
      "is_laundering\n",
      "0    312354\n",
      "1     16041\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"After SMOTETomek resampling:\\n{y_resampled.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f2afe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-03 23:26:23,174] A new study created in memory with name: no-name-f5ba6aa8-fd29-4398-b015-85e84a08b9ef\n",
      "[I 2025-05-03 23:26:29,001] Trial 0 finished with value: 0.9359357959941541 and parameters: {'n_estimators': 771, 'max_depth': 10, 'learning_rate': 0.1893095787640366, 'subsample': 0.6474281827612054, 'colsample_bytree': 0.9683321496609003, 'gamma': 0.48365372733997714, 'reg_alpha': 2.9005544746535907, 'reg_lambda': 4.409543606438956}. Best is trial 0 with value: 0.9359357959941541.\n",
      "[I 2025-05-03 23:26:29,827] Trial 1 finished with value: 0.9065506404411663 and parameters: {'n_estimators': 166, 'max_depth': 7, 'learning_rate': 0.15382540159123181, 'subsample': 0.9220893994251356, 'colsample_bytree': 0.874695162476864, 'gamma': 1.785153487959053, 'reg_alpha': 3.6746727507112595, 'reg_lambda': 6.7821199323895245}. Best is trial 0 with value: 0.9359357959941541.\n",
      "[I 2025-05-03 23:26:31,450] Trial 2 finished with value: 0.9002611642169752 and parameters: {'n_estimators': 789, 'max_depth': 7, 'learning_rate': 0.24027765769359158, 'subsample': 0.705145694443328, 'colsample_bytree': 0.6147869343819472, 'gamma': 4.396858467440954, 'reg_alpha': 9.469277584899459, 'reg_lambda': 0.36395345120288414}. Best is trial 0 with value: 0.9359357959941541.\n",
      "[I 2025-05-03 23:26:32,968] Trial 3 finished with value: 0.8987661060769778 and parameters: {'n_estimators': 717, 'max_depth': 7, 'learning_rate': 0.17756053244089767, 'subsample': 0.7224210550170066, 'colsample_bytree': 0.7763695125795936, 'gamma': 4.772383932398601, 'reg_alpha': 9.310060069267411, 'reg_lambda': 7.805615411771355}. Best is trial 0 with value: 0.9359357959941541.\n",
      "[I 2025-05-03 23:26:37,310] Trial 4 finished with value: 0.9225383222725847 and parameters: {'n_estimators': 582, 'max_depth': 10, 'learning_rate': 0.18446606242620237, 'subsample': 0.6197894513866228, 'colsample_bytree': 0.6324339963633286, 'gamma': 0.10893781577517914, 'reg_alpha': 9.989567820821865, 'reg_lambda': 6.117796467010354}. Best is trial 0 with value: 0.9359357959941541.\n",
      "[I 2025-05-03 23:26:41,603] Trial 5 finished with value: 0.9133092766857025 and parameters: {'n_estimators': 632, 'max_depth': 9, 'learning_rate': 0.03724982558353493, 'subsample': 0.7832432057140741, 'colsample_bytree': 0.6251831502076385, 'gamma': 0.047785149807691996, 'reg_alpha': 9.103819407788421, 'reg_lambda': 6.827369152234384}. Best is trial 0 with value: 0.9359357959941541.\n",
      "[I 2025-05-03 23:26:44,348] Trial 6 finished with value: 0.918700062223774 and parameters: {'n_estimators': 483, 'max_depth': 7, 'learning_rate': 0.10107464628318567, 'subsample': 0.7014902542823219, 'colsample_bytree': 0.9644372935492886, 'gamma': 0.2577407402638454, 'reg_alpha': 4.608118408088144, 'reg_lambda': 2.6711409281568335}. Best is trial 0 with value: 0.9359357959941541.\n",
      "[I 2025-05-03 23:26:45,861] Trial 7 finished with value: 0.895298006025238 and parameters: {'n_estimators': 250, 'max_depth': 8, 'learning_rate': 0.027083213721486928, 'subsample': 0.729669564132618, 'colsample_bytree': 0.8100753337245531, 'gamma': 3.1558826654180017, 'reg_alpha': 7.067609617681177, 'reg_lambda': 3.154628394485065}. Best is trial 0 with value: 0.9359357959941541.\n",
      "[I 2025-05-03 23:26:47,854] Trial 8 finished with value: 0.894034917315222 and parameters: {'n_estimators': 702, 'max_depth': 3, 'learning_rate': 0.10945252843141978, 'subsample': 0.9889260161649348, 'colsample_bytree': 0.8279185961815967, 'gamma': 1.0884982676332084, 'reg_alpha': 2.2432495973134836, 'reg_lambda': 4.897125746868136}. Best is trial 0 with value: 0.9359357959941541.\n",
      "[I 2025-05-03 23:26:49,473] Trial 9 finished with value: 0.889885259210042 and parameters: {'n_estimators': 640, 'max_depth': 3, 'learning_rate': 0.09651380431114943, 'subsample': 0.8538486504129705, 'colsample_bytree': 0.9187219091317561, 'gamma': 4.795878798992096, 'reg_alpha': 6.784199925991832, 'reg_lambda': 7.893902426445754}. Best is trial 0 with value: 0.9359357959941541.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from Optuna:\n",
      " {'n_estimators': 771, 'max_depth': 10, 'learning_rate': 0.1893095787640366, 'subsample': 0.6474281827612054, 'colsample_bytree': 0.9683321496609003, 'gamma': 0.48365372733997714, 'reg_alpha': 2.9005544746535907, 'reg_lambda': 4.409543606438956, 'objective': 'binary:logistic', 'eval_metric': 'logloss', 'n_jobs': -1, 'random_state': 42, 'tree_method': 'hist', 'verbosity': 0}\n"
     ]
    }
   ],
   "source": [
    "# Split for Optuna optimization\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled\n",
    ")\n",
    "\n",
    "# Bayesian Optimization with Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "        'n_jobs': -1,\n",
    "        'random_state': 42,\n",
    "        'tree_method': 'hist',\n",
    "        'verbosity': 0\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    return f1_score(y_val, preds, average='macro')\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_params = study.best_params\n",
    "best_params.update({\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 42,\n",
    "    'tree_method': 'hist',\n",
    "    'verbosity': 0\n",
    "})\n",
    "\n",
    "model = xgb.XGBClassifier(**best_params)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "print(\"Best Parameters from Optuna:\\n\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efcdd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report on Dataset[1]:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9998    0.9139    0.9549   6920484\n",
      "           1     0.0040    0.6668    0.0079      3565\n",
      "\n",
      "    accuracy                         0.9138   6924049\n",
      "   macro avg     0.5019    0.7903    0.4814   6924049\n",
      "weighted avg     0.9993    0.9138    0.9544   6924049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare Test Set\n",
    "test_df = feature_engineering(datasets[1])\n",
    "X_test_iso = test_df[iso_features]\n",
    "X_test_iso_transformed = iso_pipeline.named_steps['preprocess'].transform(X_test_iso)\n",
    "\n",
    "test_df['anomaly_score'] = iso_pipeline.named_steps['iso_forest'].decision_function(X_test_iso_transformed)\n",
    "test_df['anomaly_label'] = iso_pipeline.named_steps['iso_forest'].predict(X_test_iso_transformed)\n",
    "test_df['anomaly_label'] = test_df['anomaly_label'].map({1: 'Normal', -1: 'Anomaly'})\n",
    "test_df['hybrid_label'] = test_df['anomaly_label'].map({'Anomaly': 1, 'Normal': 0})\n",
    "\n",
    "X_test = test_df[hybrid_features]\n",
    "y_test = test_df['is_laundering']\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Report\n",
    "print(\"Classification Report on Dataset[1]:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
